{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6981ab-2d9a-4280-923f-235a166855ba",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning Qwen-Chat(v2.5) Large Language Model (Multiple GPUs)\n",
    "\n",
    "Tongyi Qianwen is a large language model developed by Alibaba Cloud based on the Transformer architecture, trained on an extensive set of pre-training data. The pre-training data is diverse and covers a wide range, including a large amount of internet text, specialized books, code, etc. In addition, an AI assistant called Qwen-Chat has been created based on the pre-trained model using alignment mechanism.\n",
    "\n",
    "This notebook uses Qwen-1.8B-Chat as an example to introduce how to LoRA fine-tune the Qianwen model using Deepspeed.\n",
    "\n",
    "\n",
    "## 1.Preparation\n",
    "\n",
    "### 1.1 Download Qwen2.5-1.5B-Chat(Instruct)\n",
    "\n",
    "First, download the model files. You can choose to download directly from ModelScope or huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248488f9-4a86-4f35-9d56-50f8e91a8f11",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download \"Qwen/Qwen2.5-1.5B-Instruct\" --local-dir qwen2.5-1.5b-ins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c291f0",
   "metadata": {},
   "source": [
    "### 1.2 Download the dataset\n",
    "There are massive datasets generated by R1, e.g.\n",
    "- bespokelabs/Bespoke-Stratos-17k\n",
    "- bespokelabs/Bespoke-Stratos-35k\n",
    "- NovaSky-AI/Sky-T1_data_17k\n",
    "- open-thoughts/OpenThoughts-114k\n",
    "- (You can merge these datasets to achieve better results. And less overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca950d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download \"bespokelabs/Bespoke-Stratos-17k\" --local-dir dataset_stratos_17k --repo-type dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"qwen2.5-1.5b-ins\" # provide here to merge the model after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9afd2",
   "metadata": {},
   "source": [
    "## 2. Process the dataset(Not necessary! Just to remove the **special token** <|begin_of_solution|> and <|end of solution|>)\n",
    "<br>\n",
    "Remove the <|begin_of_solution|> and <|end_of_solution|>, but keep the <|begin_of_thought|> and <|end_of_thought|>\n",
    "<br>\n",
    "If you have previously processed or merged the datasets, you will need to modify a line of code in `simplify_dataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python simplify_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20468ea9",
   "metadata": {},
   "source": [
    "## 3. Add special token to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25bc7d",
   "metadata": {},
   "source": [
    "### 3.1 With huggingface official method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aede56",
   "metadata": {},
   "source": [
    "you can add the special token `<|begin_of_thought|>` and `<|end_of_thought|>` with python\n",
    "Ref: https://huggingface.co/docs/transformers/en/main_classes/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "thought_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    extra_special_tokens={\"thought_begin\": \"<|begin_of_thought|>\", \"thought_end\": \"<|end_of_thought|>\", \"begin_solution\": \"<|begin_of_solution|>\", \"end_solution\": '<|end_of_solution|>'}\n",
    ")\n",
    "print(thought_tokenizer.thought_begin, thought_tokenizer.thought_begin_id) # should output 151665 with <|begin_of_thought|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97e442",
   "metadata": {},
   "source": [
    "Save the tokenizer(special token added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "thought_tokenizer.save_pretrained(\"output_qwen_merged\") # your final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bee22a",
   "metadata": {},
   "source": [
    "### 3.2 Or, Alternatively you can add the special token by manually editing the tokenizer_config.json\n",
    "Not ideal, but feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90397ce",
   "metadata": {},
   "source": [
    "## 4. Launch the training process\n",
    "### ! ! ! Remember to modify your config here, i.e. **model_name_or_path, data_path, nproc_per_node, per_device_train_batch_size, model_max_length...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0581e-be85-45e6-a5b7-af9c42ea697b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node 2 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 6601 finetune.py \\\n",
    "    --model_name_or_path \"qwen2.5-1.5b-ins\" \\\n",
    "    --data_path \"optimized_dataset\" \\\n",
    "    --bf16 True \\\n",
    "    --output_dir \"output_qwen\" \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 500 \\\n",
    "    --save_total_limit 3 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --adam_beta2 0.95 \\\n",
    "    --warmup_ratio 0.01 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --report_to \"none\" \\\n",
    "    --model_max_length 8192 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True \\\n",
    "    --deepspeed \"ds_config_zero2.json\" \\\n",
    "    --use_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35acf008-1dfe-4d32-8cf5-7022e042aadb",
   "metadata": {},
   "source": [
    "## 5. Merge Weights(Merge the Qwen Lora Adapters to the Qwen model) and save tokenizer\n",
    "\n",
    "The training of both LoRA and Q-LoRA only saves the adapter parameters. You can load the fine-tuned model and merge weights as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61021499-4a44-45af-a682-943ed63c2fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"qwen2-5-14b\", torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(model, \"output_qwen/\")\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"output_qwen_merged\", max_shard_size=\"4096MB\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfbd261-6451-4532-82e8-3ae19ed93ee1",
   "metadata": {},
   "source": [
    "The tokenizer files are not saved in the new directory in this step. You can copy the tokenizer files or use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcba069-340b-4a93-a145-2028b425dd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output_qwen_merged/tokenizer_config.json',\n",
       " 'output_qwen_merged/special_tokens_map.json',\n",
       " 'output_qwen_merged/vocab.json',\n",
       " 'output_qwen_merged/merges.txt',\n",
       " 'output_qwen_merged/added_tokens.json',\n",
       " 'output_qwen_merged/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\"output_qwen_merged\") # your final model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
